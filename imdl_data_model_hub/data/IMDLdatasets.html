<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.20" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const useChoice = localStorage.getItem('vuepress-color-scheme')
      const systemStatus =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (useChoice === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (useChoice === 'dark' || systemStatus) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <link rel="alternate" hreflang="zh-cn" href="https://scu-zjz.github.io/IMDLBenCo-doc/IMDLBenCo-doc/zh/imdl_data_model_hub/data/IMDLdatasets.html"><meta property="og:url" content="https://scu-zjz.github.io/IMDLBenCo-doc/IMDLBenCo-doc/imdl_data_model_hub/data/IMDLdatasets.html"><meta property="og:site_name" content="IMDLBenCo Documentation"><meta property="og:title" content="Tampering Detection Dataset Index"><meta property="og:description" content="Tampering Detection Dataset Index 1 CASIA v1.0 and CASIA v2.0 1.1 Basic Information Introduction: Both CASIA datasets are provided by the Institute of Automation, Chinese Academ..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:locale:alternate" content="zh-CN"><meta property="og:updated_time" content="2025-04-09T08:18:41.000Z"><meta property="article:modified_time" content="2025-04-09T08:18:41.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Tampering Detection Dataset Index","image":[""],"dateModified":"2025-04-09T08:18:41.000Z","author":[]}</script><link rel="icon" type="image/png" sizes="16x16" href="/images/icons/favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/icons/favicon-32x32.png"><link rel="manifest" href="/manifest.webmanifest"><meta name="application-name" content="IMDL-BenCo-documentation"><meta name="apple-mobile-web-app-title" content="IMDL-BenCo-documentation"><meta name="theme-color" content="#3eaf7c"><title>Tampering Detection Dataset Index | IMDLBenCo Documentation</title><meta name="description" content="Tampering Detection Dataset Index 1 CASIA v1.0 and CASIA v2.0 1.1 Basic Information Introduction: Both CASIA datasets are provided by the Institute of Automation, Chinese Academ...">
    <link rel="preload" href="/IMDLBenCo-doc/assets/style-Dd0dO5L-.css" as="style"><link rel="stylesheet" href="/IMDLBenCo-doc/assets/style-Dd0dO5L-.css">
    <link rel="modulepreload" href="/IMDLBenCo-doc/assets/app-CCtLKDl1.js"><link rel="modulepreload" href="/IMDLBenCo-doc/assets/IMDLdatasets.html-B06hwqaK.js">
    <link rel="prefetch" href="/IMDLBenCo-doc/assets/index.html-D1-V_O_e.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/get-started.html-BySsCoXV.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/intro.html-CTLD3-xZ.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/index.html-Ltk5iZSH.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/get-started.html-Du-auI9f.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/framework.html-DvhtPtxk.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/introduction.html-C8ewBd5h.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/0_dataprepare.html-CSVVCvpd.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/1_model_zoo.html-D67d8n6h.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/2_load_ckpt.html-Fe24u58U.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/3_demo.html-AY7YtJwI.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/4_save_images.html-C9snfzWV.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/5_complexity.html-Cjn6yL9g.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/install.html-BA0nqygR.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/AIGCdatasets.html-BnyyaxtT.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/benco.html-BUWKnMgN.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/general.html-BLpKSzMz.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/intro_content.html-C53QWo5o.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/intro_content.html-DzeQwkB3.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/iml_vit.html-B_a0c9AE.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/intro_content.html-CgPhG128.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/leaderboard.html-CXwkUSdN.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/mvss.html-DoyU83UI.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/trufor.html-DJaT23KN.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/intro.html-BVlSKECe.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/framework.html-DrQ5AsfQ.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/introduction.html-CL40lupl.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/0_dataprepare.html-kmkb_tBu.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/1_model_zoo.html-CENoS2PI.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/2_load_ckpt.html-C2ZvpUOp.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/3_demo.html-CUlC-x4j.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/4_save_images.html-CWoo8bCT.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/5_complexity.html-BbN96n_Z.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/install.html-_Y49C-Ki.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/AIGCdatasets.html-CV9Shz43.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/IMDLdatasets.html--gANHPHT.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/benco.html-DNsTNFTa.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/general.html-B7608Qus.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/intro_content.html-goNDfp2E.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/intro_content.html-Du0noCEF.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/iml_vit.html-DsR6AGho.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/intro_content.html-CYinAQrJ.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/leaderboard.html-ChHSGMHK.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/mvss.html-BLYN1at3.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/trufor.html-D9327B8y.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/404.html-Bg-7ySB5.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/index-B-M8YVCw.js" as="script"><link rel="prefetch" href="/IMDLBenCo-doc/assets/giscus-BwIGYrs0.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/IMDLBenCo-doc/"><img class="vp-site-logo" src="/IMDLBenCo-doc/images/IMDL_BenCo.png" alt="IMDLBenCo Documentation"><span class="vp-site-name vp-hide-mobile" aria-hidden="true">IMDLBenCo Documentation</span></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="Guide"><span class="title">Guide</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="Guide"><span class="title">Guide</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><!--[--><h4 class="vp-navbar-dropdown-subtitle"><span>Basic Information</span></h4><ul class="vp-navbar-dropdown-subitem-wrapper"><!--[--><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/base/introduction.html" aria-label="Introduction"><!--[--><!--[--><!--]--><!--]-->Introduction<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/base/framework.html" aria-label="Framework Design"><!--[--><!--[--><!--]--><!--]-->Framework Design<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><li class="vp-navbar-dropdown-item"><!--[--><h4 class="vp-navbar-dropdown-subtitle"><span>Quick Start</span></h4><ul class="vp-navbar-dropdown-subitem-wrapper"><!--[--><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/install.html" aria-label="Installation"><!--[--><!--[--><!--]--><!--]-->Installation<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/0_dataprepare.html" aria-label="Dataset Preparation"><!--[--><!--[--><!--]--><!--]-->Dataset Preparation<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/1_model_zoo.html" aria-label="Case One: Reproduce SoTA Papers by Training with Model Zoo"><!--[--><!--[--><!--]--><!--]-->Case One: Reproduce SoTA Papers by Training with Model Zoo<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/2_load_ckpt.html" aria-label="Case Two: Using Model Zoo with Checkpoint for Quick Testing"><!--[--><!--[--><!--]--><!--]-->Case Two: Using Model Zoo with Checkpoint for Quick Testing<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/3_demo.html" aria-label="Case Three: Implementing Your Own Model with benco init"><!--[--><!--[--><!--]--><!--]-->Case Three: Implementing Your Own Model with benco init<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/4_save_images.html" aria-label="Case Four: Inference and Save a Dataset&#39;s Mask and Label"><!--[--><!--[--><!--]--><!--]-->Case Four: Inference and Save a Dataset&#39;s Mask and Label<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/5_complexity.html" aria-label="Case Five: Obtaining Model Parameters and FLOPs"><!--[--><!--[--><!--]--><!--]-->Case Five: Obtaining Model Parameters and FLOPs<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="API Reference"><span class="title">API Reference</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="API Reference"><span class="title">API Reference</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/IMDLBenCo-doc/API/intro.html" aria-label="简介"><!--[--><!--[--><!--]--><!--]-->简介<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="IMDL data &amp; model hub"><span class="title">IMDL data &amp; model hub</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="IMDL data &amp; model hub"><span class="title">IMDL data &amp; model hub</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><!--[--><h4 class="vp-navbar-dropdown-subtitle"><span>Datasets</span></h4><ul class="vp-navbar-dropdown-subitem-wrapper"><!--[--><li class="vp-navbar-dropdown-subitem"><a class="route-link route-link-active auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/data/IMDLdatasets.html" aria-label="Tampering Detection Dataset Index"><!--[--><!--[--><!--]--><!--]-->Tampering Detection Dataset Index<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/data/AIGCdatasets.html" aria-label="AIGC Generated Content Dataset Index"><!--[--><!--[--><!--]--><!--]-->AIGC Generated Content Dataset Index<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><li class="vp-navbar-dropdown-item"><!--[--><h4 class="vp-navbar-dropdown-subtitle"><span>Models &amp; Papers</span></h4><ul class="vp-navbar-dropdown-subitem-wrapper"><!--[--><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/models/benco.html" aria-label="Models Implemented in BenCo"><!--[--><!--[--><!--]--><!--]-->Models Implemented in BenCo<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/models/general.html" aria-label="Other Models, Algorithms, Papers"><!--[--><!--[--><!--]--><!--]-->Other Models, Algorithms, Papers<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="Select language"><span class="title">Languages</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="Select language"><span class="title">Languages</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link route-link-active auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/data/IMDLdatasets.html" aria-label="English"><!--[--><!--[--><!--]--><!--]-->English<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/IMDLBenCo-doc/zh/imdl_data_model_hub/data/IMDLdatasets.html" aria-label="简体中文"><!--[--><!--[--><!--]--><!--]-->简体中文<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="vp-navbar-item"><a class="auto-link external-link" href="https://github.com/scu-zjz/IMDLBenCo" aria-label="GitHub" rel="noopener noreferrer" target="_blank"><!--[--><!--[--><!--]--><!--]-->GitHub<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="Guide"><span class="title">Guide</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="Guide"><span class="title">Guide</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><!--[--><h4 class="vp-navbar-dropdown-subtitle"><span>Basic Information</span></h4><ul class="vp-navbar-dropdown-subitem-wrapper"><!--[--><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/base/introduction.html" aria-label="Introduction"><!--[--><!--[--><!--]--><!--]-->Introduction<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/base/framework.html" aria-label="Framework Design"><!--[--><!--[--><!--]--><!--]-->Framework Design<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><li class="vp-navbar-dropdown-item"><!--[--><h4 class="vp-navbar-dropdown-subtitle"><span>Quick Start</span></h4><ul class="vp-navbar-dropdown-subitem-wrapper"><!--[--><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/install.html" aria-label="Installation"><!--[--><!--[--><!--]--><!--]-->Installation<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/0_dataprepare.html" aria-label="Dataset Preparation"><!--[--><!--[--><!--]--><!--]-->Dataset Preparation<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/1_model_zoo.html" aria-label="Case One: Reproduce SoTA Papers by Training with Model Zoo"><!--[--><!--[--><!--]--><!--]-->Case One: Reproduce SoTA Papers by Training with Model Zoo<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/2_load_ckpt.html" aria-label="Case Two: Using Model Zoo with Checkpoint for Quick Testing"><!--[--><!--[--><!--]--><!--]-->Case Two: Using Model Zoo with Checkpoint for Quick Testing<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/3_demo.html" aria-label="Case Three: Implementing Your Own Model with benco init"><!--[--><!--[--><!--]--><!--]-->Case Three: Implementing Your Own Model with benco init<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/4_save_images.html" aria-label="Case Four: Inference and Save a Dataset&#39;s Mask and Label"><!--[--><!--[--><!--]--><!--]-->Case Four: Inference and Save a Dataset&#39;s Mask and Label<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/guide/quickstart/5_complexity.html" aria-label="Case Five: Obtaining Model Parameters and FLOPs"><!--[--><!--[--><!--]--><!--]-->Case Five: Obtaining Model Parameters and FLOPs<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="API Reference"><span class="title">API Reference</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="API Reference"><span class="title">API Reference</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/IMDLBenCo-doc/API/intro.html" aria-label="简介"><!--[--><!--[--><!--]--><!--]-->简介<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="IMDL data &amp; model hub"><span class="title">IMDL data &amp; model hub</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="IMDL data &amp; model hub"><span class="title">IMDL data &amp; model hub</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><!--[--><h4 class="vp-navbar-dropdown-subtitle"><span>Datasets</span></h4><ul class="vp-navbar-dropdown-subitem-wrapper"><!--[--><li class="vp-navbar-dropdown-subitem"><a class="route-link route-link-active auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/data/IMDLdatasets.html" aria-label="Tampering Detection Dataset Index"><!--[--><!--[--><!--]--><!--]-->Tampering Detection Dataset Index<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/data/AIGCdatasets.html" aria-label="AIGC Generated Content Dataset Index"><!--[--><!--[--><!--]--><!--]-->AIGC Generated Content Dataset Index<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><li class="vp-navbar-dropdown-item"><!--[--><h4 class="vp-navbar-dropdown-subtitle"><span>Models &amp; Papers</span></h4><ul class="vp-navbar-dropdown-subitem-wrapper"><!--[--><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/models/benco.html" aria-label="Models Implemented in BenCo"><!--[--><!--[--><!--]--><!--]-->Models Implemented in BenCo<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-subitem"><a class="route-link auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/models/general.html" aria-label="Other Models, Algorithms, Papers"><!--[--><!--[--><!--]--><!--]-->Other Models, Algorithms, Papers<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="Select language"><span class="title">Languages</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="Select language"><span class="title">Languages</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link route-link-active auto-link" href="/IMDLBenCo-doc/imdl_data_model_hub/data/IMDLdatasets.html" aria-label="English"><!--[--><!--[--><!--]--><!--]-->English<!--[--><!--[--><!--]--><!--]--></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/IMDLBenCo-doc/zh/imdl_data_model_hub/data/IMDLdatasets.html" aria-label="简体中文"><!--[--><!--[--><!--]--><!--]-->简体中文<!--[--><!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="vp-navbar-item"><a class="auto-link external-link" href="https://github.com/scu-zjz/IMDLBenCo" aria-label="GitHub" rel="noopener noreferrer" target="_blank"><!--[--><!--[--><!--]--><!--]-->GitHub<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><p tabindex="0" class="vp-sidebar-item vp-sidebar-heading active">IMDL Data &amp; Model hub <!----></p><ul style="" class="vp-sidebar-children"><!--[--><li><p tabindex="0" class="vp-sidebar-item active">Datasets <!----></p><ul style="" class="vp-sidebar-children"><!--[--><li><a class="route-link route-link-active auto-link vp-sidebar-item active" href="/IMDLBenCo-doc/imdl_data_model_hub/data/IMDLdatasets.html" aria-label="Tampering Detection Dataset Index"><!--[--><!--[--><!--]--><!--]-->Tampering Detection Dataset Index<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/IMDLBenCo-doc/imdl_data_model_hub/data/AIGCdatasets.html" aria-label="AIGC Generated Content Dataset Index"><!--[--><!--[--><!--]--><!--]-->AIGC Generated Content Dataset Index<!--[--><!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><li><p tabindex="0" class="vp-sidebar-item">Models &amp; Papers <!----></p><ul style="" class="vp-sidebar-children"><!--[--><li><a class="route-link auto-link vp-sidebar-item" href="/IMDLBenCo-doc/imdl_data_model_hub/models/benco.html" aria-label="Models Implemented in BenCo"><!--[--><!--[--><!--]--><!--]-->Models Implemented in BenCo<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/IMDLBenCo-doc/imdl_data_model_hub/models/general.html" aria-label="Other Models, Algorithms, Papers"><!--[--><!--[--><!--]--><!--]-->Other Models, Algorithms, Papers<!--[--><!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div vp-content><!--[--><!--]--><div><h1 id="tampering-detection-dataset-index" tabindex="-1"><a class="header-anchor" href="#tampering-detection-dataset-index"><span>Tampering Detection Dataset Index</span></a></h1><table><thead><tr><th>Dataset Name</th><th>Genuine/Tampered Image Count</th><th>Minimum Resolution</th><th>Maximum Resolution</th><th>Image Features Summary</th><th>Current Download Links</th></tr></thead><tbody><tr><td>CASIA v1.0</td><td>800/921</td><td>384x256</td><td>384x256</td><td>Primarily targeting splice operations.</td><td><a href="https://github.com/namtpham/casia1groundtruth" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>CASIA v2.0</td><td>7491/5123</td><td>320x240</td><td>800x600</td><td>Distinguishes between copy-move and splice tampering methods.</td><td><a href="https://github.com/namtpham/casia2groundtruth" target="_blank" rel="noopener noreferrer">Link</a><br><a href="https://github.com/SunnyHaze/CASIA2.0-Corrected-Groundtruth" target="_blank" rel="noopener noreferrer">Corrected Version Link</a></td></tr><tr><td>Columbia</td><td>Black and White Dataset: 933/921<br>Color Dataset: 183/180</td><td>Black and White Dataset: 128x128<br>Color Dataset: 757x568</td><td>Black and White Dataset: 128x128<br>Color Dataset: 1152x768</td><td>Splice tampering based on uncompressed images, with higher image resolutions.</td><td><a href="">Black and White Dataset</a><br><a href="https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/" target="_blank" rel="noopener noreferrer">Color Dataset</a></td></tr><tr><td>Coverage</td><td>100/100</td><td>334x190</td><td>752x472</td><td>Targeting copy-move tampering operations, typically copying one item from a group of similar items.</td><td><a href="https://github.com/wenbihan/coverage" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>NIST16</td><td>0/564</td><td>5616x3744</td><td>500x500</td><td>Including splice, remove, and copy-move tampering, with high-resolution images in the dataset.</td><td><a href="https://mig.nist.gov/MFC/PubData/Resources.html" target="_blank" rel="noopener noreferrer">Link</a><br><a href="https://pan.baidu.com/share/init?surl=nZVUaqaBOD1u3xtxVDKmFw" target="_blank" rel="noopener noreferrer">Cloud Disk Link</a></td></tr><tr><td>Defacto</td><td></td><td></td><td></td><td>Large dataset with tampering areas occupying a very small proportion of the total image area.</td><td><a href="https://www.kaggle.com/defactodataset/datasets" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>IMD2020</td><td>414/2020</td><td>260x193</td><td>2958x4437</td><td>Tampering types include complex real-world editing (such as splicing, local modification), with variable resolutions.</td><td><a href="https://staff.utia.cas.cz/novozada/db/IMD2020.zip" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>FantasticReality</td><td>16592/19423</td><td>500x333</td><td>6000x4000</td><td>A multi-task annotated dataset combining tampering localization and semantic segmentation, providing pixel-level tampering area masks, instance segmentation, and category labels.</td><td><a href="https://drive.google.com/drive/folders/1AIKseHoL0ebux7tyeBDWN5zkHzAgYBzJ?usp=sharing" target="_blank" rel="noopener noreferrer">Link1</a><br><a href="https://github.com/mjkwon2021/CAT-Net/issues/51" target="_blank" rel="noopener noreferrer">Link2</a></td></tr><tr><td>PhotoShop-battle</td><td>11142/91886</td><td>Height: 136<br>Width: 68</td><td>Height: 20000<br>Width: 12024</td><td>A large-scale real-world dataset for creative tampering detection, with a wide range of image resolutions.</td><td><a href="https://github.com/dbisUnibas/PS-Battles" target="_blank" rel="noopener noreferrer">Link1</a><br><a href="https://www.kaggle.com/datasets/timocasti/psbattles/data" target="_blank" rel="noopener noreferrer">Link2</a></td></tr><tr><td>Carvalho（DSO-1）</td><td>100/100</td><td>2048x1536</td><td>2048x1536</td><td>Forging through splicing by adding people and other auxiliary processing.</td><td><a href="http://ic.unicamp.br/~rocha/pub/downloads/2014-tiago-carvalho-thesis/tifs-database.zip" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>GRIP Dataset</td><td>80/80</td><td>1024x768</td><td>1024x768</td><td>Designed to assess the robustness of copy-move tampering detection algorithms under complex post-processing. The tampered area in the dataset is small.</td><td><a href="https://www.grip.unina.it/download/prog/CMFD/" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>CoMoFoD</td><td>260/260</td><td>512x512</td><td>3000x3000</td><td>Designed specifically for the evaluation of copy-move detection algorithms, with various types of post-processing methods applied to both forged and original images.</td><td><a href="https://www.vcl.fer.hr/comofod/download.html" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>CocoGlide</td><td>512/512</td><td>256x256</td><td>256x256</td><td>Targeting generative tampering research, combining GLIDE diffusion models with semantic prompts to generate tampered content, simulating semantic-level local tampering.</td><td><a href="https://www.grip.unina.it/download/prog/TruFor/CocoGlide.zip" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>tampCOCO</td><td>0/800000</td><td>72x51</td><td>640x640</td><td>Built based on the COCO 2017 dataset, including copy-move and splice, all images are JPEG compressed, retaining clear boundaries to support model learning of low-level tampering traces.</td><td><a href="https://www.kaggle.com/datasets/qsii24/tampcoco" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>compRAISE</td><td>24462/0</td><td>2278x1515</td><td>6159x4079</td><td>Built based on a high-resolution image library and COCO instance annotations. Copy-move forgeries with irregular shapes, non-rectangular and asymmetric.</td><td><a href="https://www.kaggle.com/datasets/qsii24/compraise" target="_blank" rel="noopener noreferrer">Link</a></td></tr><tr><td>OpenForensics</td><td>0/2000</td><td>512x512</td><td>512x512</td><td>The first large-scale dataset for multi-face forgery detection and segmentation. Rich content, image scenes cover indoor and outdoor; diverse face situations, size variations.</td><td><a href="https://zenodo.org/records/5528418" target="_blank" rel="noopener noreferrer">Link</a></td></tr></tbody></table><h2 id="_1-casia-v1-0-and-casia-v2-0" tabindex="-1"><a class="header-anchor" href="#_1-casia-v1-0-and-casia-v2-0"><span>1 CASIA v1.0 and CASIA v2.0</span></a></h2><h3 id="_1-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_1-1-basic-information"><span>1.1 Basic Information</span></a></h3><ul><li>Introduction: Both CASIA datasets are provided by the <strong>Institute of Automation, Chinese Academy of Sciences</strong> and are primarily aimed at detecting Splicing operations. Notably, the official site does not provide 0-1 Masks as ground truth; only tampered and original images are available! <ul><li>CASIA V1.0 includes 921 tampered images and their corresponding original images, with a fixed resolution of 384x256, and the tampering method is limited to Splicing.</li><li>CASIA V2.0 includes 5123 tampered images and their corresponding original images, with resolutions ranging from 320x240 to 800x600, and in addition to Splicing, blurring is also used.</li></ul></li><li>APA citation: Dong, J., Wang, W., &amp; Tan, T. (2013). CASIA Image Tampering Detection Evaluation Database. 2013 IEEE China Summit and International Conference on Signal and Information Processing, 422–426. https://doi.org/10.1109/ChinaSIP.2013.6625374</li><li>Paper link: <a href="https://ieeexplore.ieee.org/abstract/document/6625374" target="_blank" rel="noopener noreferrer">CASIA Image Tampering Detection Evaluation Database | IEEE Conference Publication | IEEE Xplore</a></li></ul><h3 id="_1-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_1-2-download-and-usage"><span>1.2 Download and Usage</span></a></h3><p>​ It should be noted that the original website of the Institute of Automation, Chinese Academy of Sciences, is currently under maintenance and cannot be used. There are some naming-related annotation errors in the dataset, and the original dataset does not provide 0-1 masks as GroundTruth.</p><p>​ To summarize, the widely used CASIA datasets are those provided by &quot;Pham&quot; and others in a 2019 paper titled &quot;Hybrid Image-Retrieval Method for Image-Splicing Validation,&quot; which are open-sourced with 0-1 masks. The authors themselves used algorithms to subtract the original images from the tampered images to obtain datasets with 0-1 masks and corresponding images, and corrected file name-related errors. These datasets are also widely circulated on data science platforms such as Kaggle, and the GitHub links provided by Pham and others are as follows:</p><ul><li>CASIA v1.0: https://github.com/namtpham/casia1groundtruth</li><li>CASIA v2.0: https://github.com/namtpham/casia2groundtruth</li></ul><p>​ However, the CASIA v2.0 corrected by Pham and others still has some vulnerabilities. Based on this dataset, under the guidance of Professor Zhou Jizhe from DICALAB, student Ma Xiaochen made corrections to some of the errors and uploaded a new corrected dataset. Apart from changes to dozens of images, the rest of the information is identical to the dataset provided by Pham and others, which can be downloaded through the following GitHub link:</p><ul><li>Corrected CASIA v2.0: https://github.com/SunnyHaze/CASIA2.0-Corrected-Groundtruth</li></ul><h2 id="_2-columbia" tabindex="-1"><a class="header-anchor" href="#_2-columbia"><span>2 Columbia</span></a></h2><h3 id="_2-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_2-1-basic-information"><span>2.1 Basic Information</span></a></h3><ul><li><p>Introduction: The Columbia dataset is a tampering detection dataset produced by Columbia University&#39;s DVMM Lab, also primarily aimed at Splicing operations. It should be noted that there are two different versions of the dataset on the official website, one of which is a black and white fixed-size (128 x 128) dataset, and the other is a high-definition color splicing dataset, <strong>the color one is generally used as the Benchmark.</strong></p><ul><li>The black and white image block dataset contains more than 1800 128x128 image blocks.</li><li>The color Splicing dataset contains 180 image blocks with resolutions ranging from 757x568 to 1152x768.</li></ul></li><li><p>Paper link: Generally, tampering image-related papers only cite the dataset&#39;s official website, but in principle, the authors require citation of their paper: &quot;Detecting Image Splicing Using Geometry Invariants And Camera Characteristics Consistency&quot; (<a href="https://www.ee.columbia.edu/ln/dvmm/publications/06/hsu06ICMEcrf.pdf" target="_blank" rel="noopener noreferrer">hsu06ICMEcrf.pdf (columbia.edu)</a>).</p></li><li><p>APA citation: Hsu, Y., &amp; Chang, S. (2006). Detecting Image Splicing using Geometry Invariants and Camera Characteristics Consistency. 2006 IEEE International Conference on Multimedia and Expo, 549–552. https://doi.org/10.1109/ICME.2006.262447</p></li></ul><h3 id="_2-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_2-2-download-and-usage"><span>2.2 Download and Usage</span></a></h3><ul><li><p>Columbia Image Splicing Detection Evaluation Dataset (Black and White Image Block Dataset): https://www.ee.columbia.edu/ln/dvmm/downloads/AuthSplicedDataSet/AuthSplicedDataSet.htm</p></li><li><p>Columbia Uncompressed Image Splicing Detection Evaluation Dataset (Color Splicing Dataset): https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/</p></li></ul><h2 id="_3-coverage" tabindex="-1"><a class="header-anchor" href="#_3-coverage"><span>3 Coverage</span></a></h2><h3 id="_3-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_3-1-basic-information"><span>3.1 Basic Information</span></a></h3><ul><li><p>Introduction: Includes 100 pairs of tampered and corresponding genuine images, primarily applying Copy-move tampering operations. The images are highly deceptive, often involving copying one item from a group of similar items.</p></li><li><p>Paper: COVERAGE – A NOVEL DATABASE FOR COPY-MOVE FORGERY DETECTION</p></li><li><p>APA citation: Wen, B., Zhu, Y., Subramanian, R., Ng, T.-T., Shen, X., &amp; Winkler, S. (2016). COVERAGE — A novel database for copy-move forgery detection. 2016 IEEE International Conference on Image Processing (ICIP), 161–165. https://doi.org/10.1109/ICIP.2016.7532339</p></li></ul><h3 id="_3-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_3-2-download-and-usage"><span>3.2 Download and Usage</span></a></h3><ul><li>The official GitHub link: https://github.com/wenbihan/coverage</li></ul><h2 id="_4-nist16" tabindex="-1"><a class="header-anchor" href="#_4-nist16"><span>4 NIST16</span></a></h2><h3 id="_4-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_4-1-basic-information"><span>4.1 Basic Information</span></a></h3><ul><li><p>Introduction: Includes splice, remove, and copy-move tampering, with less than 1k samples. Generally, NIST16 is used for tampering detection evaluation. The images in the dataset have high resolutions and usually require resizing or evaluation using a sliding window. It should be noted that there is a certain &quot;leakage&quot; in the dataset division, meaning that some test set images also exist in the training set, but everyone generally turns a blind eye to this issue.</p></li><li><p>Paper link: https://ieeexplore.ieee.org/abstract/document/8638296/</p></li><li><p>APA citation: Guan, H., Kozak, M., Robertson, E., Lee, Y., Yates, A. N., Delgado, A., Zhou, D., Kheyrkhah, T., Smith, J., &amp; Fiscus, J. (2019). MFC Datasets: Large-Scale Benchmark Datasets for Media Forensic Challenge Evaluation. 2019 IEEE Winter Applications of Computer Vision Workshops (WACVW), 63–72. https://doi.org/10.1109/WACVW.2019.00018</p></li></ul><h3 id="_4-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_4-2-download-and-usage"><span>4.2 Download and Usage</span></a></h3><p>The official website for this dataset is a bit cumbersome to use, requiring registration on the NIST (National Institute of Standards and Technology) website to obtain permission before downloading.</p><ul><li><p>https://mfc.nist.gov/</p></li><li><p>https://mfc.nist.gov/participant</p></li></ul><p>In theory, after a series of operations, you can access this website https://mig.nist.gov/MFC/PubData/Resources.html to download the dataset mentioned above, which is commonly referred to as NIST16 (the updated version 20 generally requires a license):</p><p>NC2016 Nimble Science Seta: NC2016 Test0613.SCI.tgz NC2016 Testset June 2013: NC2016 Test0613.tarbz2</p><p><strong>If it&#39;s not so troublesome, there is a big brother on Zhihu who has uploaded it to a cloud disk, a non-official way to obtain it as follows (not guaranteed for long-term use):</strong></p><p>Baidu Cloud Disk link: https://pan.baidu.com/share/init?surl=nZVUaqaBOD1u3xtxVDKmFw</p><p>Extraction code: lik7</p><h2 id="_5-defacto" tabindex="-1"><a class="header-anchor" href="#_5-defacto"><span>5 Defacto</span></a></h2><h3 id="_5-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_5-1-basic-information"><span>5.1 Basic Information</span></a></h3><ul><li><p>Introduction: The Defacto dataset is a tampered image dataset generated based on MSCOCO, with a very large content size, but the &quot;overall tampered area as a proportion of the total image area&quot; is very small, meaning that on average, most tampering only involves very small objects in the image, which is quite different from other datasets.</p></li><li><p>Paper link: https://ieeexplore.ieee.org/abstract/document/8903181/</p></li><li><p>APA citation: Mahfoudi, G., Tajini, B., Retraint, F., Morain-Nicolier, F., Dugelay, J. L., &amp; Pic, M. (2019). DEFACTO: Image and Face Manipulation Dataset. 2019 27th European Signal Processing Conference (EUSIPCO), 1–5. https://doi.org/10.23919/EUSIPCO.2019.8903181</p></li></ul><p>Additionally, since it is generated from COCO, the authors also require citation of the MS COCO dataset:</p><ul><li><p>Paper link: https://arxiv.org/abs/1405.0312</p></li><li><p>APA citation: Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., &amp; Dollár, P. (2015). Microsoft COCO: Common Objects in Context (arXiv:1405.0312). arXiv. http://arxiv.org/abs/1405.0312</p></li></ul><h3 id="_5-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_5-2-download-and-usage"><span>5.2 Download and Usage</span></a></h3><p>The dataset has been fully uploaded to Kaggle, so it is relatively easy to download and use. Due to its large size, the authors have divided it into several parts for download:</p><p>Inpainting:</p><p>https://www.kaggle.com/datasets/defactodataset/defactoinpainting</p><p>Copy-move:</p><p>https://www.kaggle.com/datasets/defactodataset/defactocopymove</p><p>Splicing:</p><p>https://www.kaggle.com/datasets/defactodataset/defactosplicing</p><h2 id="_6-imd2020" tabindex="-1"><a class="header-anchor" href="#_6-imd2020"><span>6 IMD2020</span></a></h2><h3 id="_6-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_6-1-basic-information"><span>6.1 Basic Information</span></a></h3><ul><li><p>Introduction: IMD2020 is a large-scale tampered image detection dataset constructed by the team from the Institute of Information Theory and Automation of the Czech Academy of Sciences, consisting of both synthetically generated and real tampered parts. The former is based on 35,000 real images taken by 2,322 different camera models, and a set of tampered images was synthesized using a large number of image manipulation techniques (including image processing technologies and GAN-based or repair methods); the latter consists of 2,000 &quot;real-life&quot; (uncontrolled) tampered images collected from the internet. Provides precise binary mask (0-1 Mask) annotations.</p><ul><li>Synthetically generated dataset: <ul><li>Includes 35,000 real images (from 2,322 camera models) and corresponding 35,000 tampered images, totaling 70,000 images.</li><li>Tampering methods include traditional processing (JPEG compression, blurring, noise, etc.), GAN generation (such as FaceApp), image repair (Inpainting), etc., with tampered area ratios ranging from 5% to 30%.</li><li>All tampered images provide binary mask annotations for the tampered areas.</li></ul></li><li>Real-life dataset: <ul><li>Includes 2,010 uncontrolled tampered images collected from the internet, each matched with the original image and manually annotated with binary masks.</li><li>Tampering types include complex real-world editing (such as splicing, local modification), with variable resolutions.</li></ul></li></ul></li><li><p>Paper link: <a href="https://ieeexplore.ieee.org/document/9096940" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/document/9096940/</a></p></li><li><p>APA citation: Novozamsky, A., Mahdian, B., &amp; Saic, S. (2020). IMD2020: A large-scale annotated dataset tailored for detecting manipulated images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision workshops (pp. 71-80). <a href="https://doi.org/10.1109/WACVW50321.2020.9096940" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/WACVW50321.2020.9096940</a></p></li></ul><h3 id="_6-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_6-2-download-and-usage"><span>6.2 Download and Usage</span></a></h3><p>Official download website: https://staff.utia.cas.cz/novozada/db/</p><ul><li><p>Real-life tampered dataset: IMD2020 Real-Life Manipulated Images section</p></li><li><p>Synthetic tampered dataset: IMD2020 Large-Scale Set of Inpainting Images section</p></li></ul><h2 id="_7-fantasticreality" tabindex="-1"><a class="header-anchor" href="#_7-fantasticreality"><span>7 FantasticReality</span></a></h2><h3 id="_7-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_7-1-basic-information"><span>7.1 Basic Information</span></a></h3><ul><li>Introduction: The FantasticReality dataset is a large-scale tampered detection dataset jointly constructed by the GosNIIAS (Russian National Research Institute for Aviation Systems), MIPT (Moscow Institute of Physics and Technology), and FBK (Bruno Kessler Foundation), aiming to address the issues of small scale and incomplete annotation in existing datasets. It also provides pixel-level tampered area masks (ground truth masks), instance segmentation, and category labels, covering 10 common object categories (such as people, vehicles, buildings, etc.), and is the first <!----> annotated dataset combining tampered localization and semantic segmentation. It includes 16k real images and 16k tampered images, totaling 32k images, with the main tampering method being Splicing.</li><li>Paper link: <a href="https://papers.nips.cc/paper_files/paper/2019/hash/98dce83da57b0395e163467c9dae521b-Abstract.html" target="_blank" rel="noopener noreferrer">The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection</a></li><li>APA citation: Kniaz, V. V., Knyaz, V., &amp; Remondino, F. (2019). The point where reality meets fantasy: Mixed adversarial generators for image splice detection. <em>Advances in neural information processing systems</em>, <em>32</em>.</li></ul><h3 id="_7-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_7-2-download-and-usage"><span>7.2 Download and Usage</span></a></h3><ul><li><p>The original paper provides a dataset download link: <a href="http://zefirus.org/MAG" target="_blank" rel="noopener noreferrer">http://zefirus.org/MAG</a> (defunct).</p></li><li><p>The authors have provided a new download link in the CAT-Net GitHub repository: <a href="https://drive.google.com/drive/folders/1AIKseHoL0ebux7tyeBDWN5zkHzAgYBzJ?usp=sharing" target="_blank" rel="noopener noreferrer">https://drive.google.com/drive/folders/1AIKseHoL0ebux7tyeBDWN5zkHzAgYBzJ?usp=sharing</a>, but it seems there are still issues.</p></li><li><p>Other options include contacting the authors for download permissions. For details, see <a href="https://github.com/mjkwon2021/CAT-Net/issues/51" target="_blank" rel="noopener noreferrer">https://github.com/mjkwon2021/CAT-Net/issues/51</a>, where the authors provide specific contact information and requirements.</p></li></ul><h2 id="_8-photoshop-battle" tabindex="-1"><a class="header-anchor" href="#_8-photoshop-battle"><span>8 PhotoShop-battle</span></a></h2><h3 id="_8-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_8-1-basic-information"><span>8.1 Basic Information</span></a></h3><ul><li>Introduction: The PS-Battles dataset, developed by the University of Basel, Switzerland, based on the Reddit community <strong>r/photoshopbattles</strong>, is the first large-scale real-world dataset for <em>creative tampering detection</em>. It focuses on diverse and high-semantic image tampering content generated by community users, including humorous composites, scene replacements, and character fusions (such as splicing, copy-move, and removal). The dataset contains 11,142 image sets (a total of 103,028 images) with a wide range of resolutions (width: 68-12,024 pixels, height: 136-20,000 pixels).</li><li>Paper link: <a href="https://arxiv.org/abs/1804.04866" target="_blank" rel="noopener noreferrer">The PS-Battles Dataset - an Image Collection for Image Manipulation Detection</a></li><li>APA citation: Heller, S., Rossetto, L., &amp; Schuldt, H. (2018). The ps-battles dataset-an image collection for image manipulation detection. <em>arXiv preprint arXiv:1804.04866</em>. <a href="https://arxiv.org/abs/1804.04866" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1804.04866</a></li></ul><h3 id="_8-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_8-2-download-and-usage"><span>8.2 Download and Usage</span></a></h3><ul><li><p>Official GitHub repository: <a href="https://github.com/dbisUnibas/PS-Battles" target="_blank" rel="noopener noreferrer">https://github.com/dbisUnibas/PS-Battles</a></p></li><li><p>Kaggle: <a href="https://www.kaggle.com/datasets/timocasti/psbattles/data" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/timocasti/psbattles/data</a></p><ul><li>For Ubuntu and MacOS: See the GitHub repository for details. Run the provided <code>download.sh</code> script.</li><li>For Windows: See the Kaggle link. Place the provided <code>download.py</code> script and <code>Originals.tsv</code> &amp; <code>photoshops.tsv</code> in the same directory, then run the <code>download.py</code> script.</li></ul></li></ul><h2 id="_9-carvalho-dso-1" tabindex="-1"><a class="header-anchor" href="#_9-carvalho-dso-1"><span>9 Carvalho (DSO-1)</span></a></h2><h3 id="_9-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_9-1-basic-information"><span>9.1 Basic Information</span></a></h3><ul><li>Introduction: The DSO-1 dataset contains 200 high-resolution images (2048×1536 pixels), with 100 original unmodified images and 100 forged images. Forgery is achieved by <em>splicing to add people</em> (adding one or more people to a source image that already contains one or more people) with adjustments in color and brightness.</li><li>Paper link: <a href="https://ieeexplore.ieee.org/document/6522874" target="_blank" rel="noopener noreferrer">Exposing Digital Image Forgeries by Illumination Color Classification</a></li><li>APA citation: De Carvalho, T. J., Riess, C., Angelopoulou, E., Pedrini, H., &amp; de Rezende Rocha, A. (2013). Exposing digital image forgeries by illumination color classification. <em>IEEE Transactions on Information Forensics and Security</em>, <em>8</em>(7), 1182-1194. <a href="https://doi.org/10.1109/TIFS.2013.2265677" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/TIFS.2013.2265677</a></li></ul><h3 id="_9-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_9-2-download-and-usage"><span>9.2 Download and Usage</span></a></h3><ul><li>Download link: <a href="http://ic.unicamp.br/%7Erocha/pub/downloads/2014-tiago-carvalho-thesis/tifs-database.zip" target="_blank" rel="noopener noreferrer">http://ic.unicamp.br/~rocha/pub/downloads/2014-tiago-carvalho-thesis/tifs-database.zip</a> (appears to be problematic and may not succeed)</li><li>Related datasets collection: <a href="https://recodbr.wordpress.com/code-n-data/#porno" target="_blank" rel="noopener noreferrer">https://recodbr.wordpress.com/code-n-data/#porno</a></li></ul><h2 id="_10-grip-dataset" tabindex="-1"><a class="header-anchor" href="#_10-grip-dataset"><span>10 GRIP Dataset</span></a></h2><h3 id="_10-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_10-1-basic-information"><span>10.1 Basic Information</span></a></h3><ul><li>Introduction: The dataset is designed to assess the robustness of <em>copy-move forgery detection</em> algorithms under complex post-processing interference. It focuses on detecting small-scale tampering regions, containing 80 images and addressing issues of large tampering regions and limited post-processing types in previous datasets. It includes high-resolution images (e.g., 768×1024 pixels) with tampering regions ranging from 4000 pixels (&lt;1%) to 50,000 pixels, categorized into three background complexities: smooth, mixed, and textured. Pixel-level ground-truth masks (0-1 masks) are provided.</li><li>Paper link: <a href="https://ieeexplore.ieee.org/document/7154457" target="_blank" rel="noopener noreferrer">Efficient Dense-Field Copy–Move Forgery Detection</a></li><li>APA citation: Cozzolino, D., Poggi, G., &amp; Verdoliva, L. (2015). Efficient dense-field copy–move forgery detection. <em>IEEE Transactions on Information Forensics and Security</em>, <em>10</em>(11), 2284-2297. <a href="https://doi.org/10.1109/TIFS.2015.2455334" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/TIFS.2015.2455334</a></li></ul><h3 id="_10-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_10-2-download-and-usage"><span>10.2 Download and Usage</span></a></h3><ul><li>The dataset is provided by the GRIP group at <a href="http://www.unina.it/" target="_blank" rel="noopener noreferrer">University Federico II of Naples</a>: <a href="https://www.grip.unina.it/download/prog/CMFD/" target="_blank" rel="noopener noreferrer">https://www.grip.unina.it/download/prog/CMFD/</a></li></ul><h2 id="_11-comofod" tabindex="-1"><a class="header-anchor" href="#_11-comofod"><span>11 CoMoFoD</span></a></h2><h3 id="_11-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_11-1-basic-information"><span>11.1 Basic Information</span></a></h3><ul><li>Introduction: CoMoFoD (Copy-Move Forgery Detection) database, developed by the University of Zagreb, Croatia, is a comprehensive benchmark dataset for <em>copy-move forgery detection</em> algorithm evaluation. It systematically integrates various geometric transformations and post-processing operations, providing pixel-level tampering mask annotations to address the limitations of existing datasets in terms of post-processing types and scale. It contains 260 sets of forged images divided into two categories: 200 sets in the small image category (512x512) and 60 sets in the large image category (3000x2000). All forged and original images have undergone different types of post-processing methods, such as JPEG compression, blurring, adding noise, and color reduction.</li><li>Paper link: <a href="https://ieeexplore.ieee.org/document/6658316" target="_blank" rel="noopener noreferrer">CoMoFoD — New database for copy-move forgery detection</a></li><li>APA citation: Tralic, D., Zupancic, I., Grgic, S., &amp; Grgic, M. (2013, September). CoMoFoD—New database for copy-move forgery detection. In <em>Proceedings ELMAR-2013</em> (pp. 49-54). IEEE.</li></ul><h3 id="_11-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_11-2-download-and-usage"><span>11.2 Download and Usage</span></a></h3><ul><li>Official dataset website: <a href="https://www.vcl.fer.hr/comofod/download.html" target="_blank" rel="noopener noreferrer">https://www.vcl.fer.hr/comofod/download.html</a><ul><li>Small image category database (512x512): 200 sets, download link: <a href="https://www.vcl.fer.hr/comofod/comofod_small.rar" target="_blank" rel="noopener noreferrer">https://www.vcl.fer.hr/comofod/comofod_small.rar</a></li><li>Large image category database (3000x2000): 60 sets, no download link available; contact the authors for access.</li></ul></li></ul><h2 id="_12-cocoglide" tabindex="-1"><a class="header-anchor" href="#_12-cocoglide"><span>12 CocoGlide</span></a></h2><h3 id="_12-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_12-1-basic-information"><span>12.1 Basic Information</span></a></h3><ul><li>Introduction: CocoGlide is a tampering detection dataset built on the COCO 2017 validation set, designed to evaluate the local tampering detection capabilities of modern generative models (e.g., diffusion models). It generates realistic tampering content by combining the GLIDE diffusion model with semantic prompts to replace corresponding regions in the original images (e.g., animals, vehicles), simulating semantic-level local tampering in real-world scenarios. It contains <strong>512 tampered images</strong>, all generated from 256×256 pixel crops of the COCO validation set, filling the gap of <em>generative tampering</em> samples in traditional datasets.</li><li>Paper link: <a href="https://arxiv.org/abs/2212.10957" target="_blank" rel="noopener noreferrer">TruFor: Leveraging all-round clues for trustworthy image forgery detection and localization</a></li><li>APA citation: Guillaro, F., Cozzolino, D., Sud, A., Dufour, N., &amp; Verdoliva, L. (2023). Trufor: Leveraging all-round clues for trustworthy image forgery detection and localization. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 20606-20615).</li></ul><h3 id="_12-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_12-2-download-and-usage"><span>12.2 Download and Usage</span></a></h3><ul><li>The dataset is provided by the GRIP group at <a href="http://www.unina.it/" target="_blank" rel="noopener noreferrer">University Federico II of Naples</a>: <a href="https://www.grip.unina.it/download/prog/TruFor/CocoGlide.zip" target="_blank" rel="noopener noreferrer">https://www.grip.unina.it/download/prog/TruFor/CocoGlide.zip</a></li></ul><h2 id="_13-tampcoco" tabindex="-1"><a class="header-anchor" href="#_13-tampcoco"><span>13 tampCOCO</span></a></h2><h3 id="_13-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_13-1-basic-information"><span>13.1 Basic Information</span></a></h3><ul><li><p>Introduction: tampCOCO is a tampering detection dataset built on the COCO 2017 dataset, consisting of two parts: SP COCO (cross-image splicing) and CM COCO (intra-image copy-move). Pixel-level binary masks (0-1 masks) are provided.</p><ul><li><strong>SP COCO</strong>: <ul><li>Based on COCO images, objects (e.g., people, vehicles) are randomly selected from one image, rotated/scaled, and pasted into a random position in another image.</li><li>A total of 200,000 forged images, all subjected to JPEG compression (quality factor 60-100) without additional blurring or other post-processing.</li></ul></li><li><strong>CM COCO</strong>: <ul><li>In a single COCO image, selected regions (e.g., objects or backgrounds) are copied and pasted to other locations to create copy-move tampering samples.</li><li>A total of 600,000 images, with JPEG compression parameters consistent with SP COCO, retaining clear boundaries to support model learning of low-level tampering traces.</li></ul></li></ul></li><li><p>Paper link: <a href="https://arxiv.org/abs/2108.12947" target="_blank" rel="noopener noreferrer">Learning JPEG Compression Artifacts for Image Manipulation Detection and Localization</a></p></li><li><p>APA citation: Kwon, M. J., Nam, S. H., Yu, I. J., Lee, H. K., &amp; Kim, C. (2022). Learning jpeg compression artifacts for image manipulation detection and localization. <em>International Journal of Computer Vision</em>, <em>130</em>(8), 1875-1895. <a href="https://arxiv.org/abs/2108.12947" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2108.12947</a></p></li></ul><h3 id="_13-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_13-2-download-and-usage"><span>13.2 Download and Usage</span></a></h3><ul><li>The dataset is fully uploaded to Kaggle: <a href="https://www.kaggle.com/datasets/qsii24/tampcoco" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/qsii24/tampcoco</a></li><li>Due to its large size, the dataset is divided into 13 parts for download. The above link directs to the index for all downloads.</li></ul><h2 id="_14-compraise" tabindex="-1"><a class="header-anchor" href="#_14-compraise"><span>14 compRAISE</span></a></h2><h3 id="_14-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_14-1-basic-information"><span>14.1 Basic Information</span></a></h3><ul><li>Introduction: compRAISE (CM RAISE) is a complex tampering detection dataset built on the <strong>RAISE high-resolution image library</strong> and <strong>COCO instance annotations</strong>. High-resolution natural scene images (resolution range: 2,000×3,008 to 4,928×3,264) are selected from the RAISE dataset (containing 8,156 uncompressed RAW images). Irregularly shaped regions are extracted using random polygon instance masks from COCO 2017 (approximately 1.2 million annotations), ensuring non-rectangular and asymmetric tampering boundaries. A <em>copy-move strategy</em> is then applied within individual RAISE images to forge tampering.</li><li>Paper link: <a href="https://arxiv.org/abs/2108.12947" target="_blank" rel="noopener noreferrer">Learning JPEG Compression Artifacts for Image Manipulation Detection and Localization</a></li><li>APA citation: Kwon, M. J., Nam, S. H., Yu, I. J., Lee, H. K., &amp; Kim, C. (2022). Learning jpeg compression artifacts for image manipulation detection and localization. <em>International Journal of Computer Vision</em>, <em>130</em>(8), 1875-1895. <a href="https://arxiv.org/abs/2108.12947" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2108.12947</a></li></ul><h3 id="_14-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_14-2-download-and-usage"><span>14.2 Download and Usage</span></a></h3><ul><li>The dataset is fully uploaded to Kaggle: <a href="https://www.kaggle.com/datasets/qsii24/compraise" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/qsii24/compraise</a></li><li>Due to its large size, the dataset is divided into 15 parts for download. The above link directs to the index for all downloads.</li></ul><h2 id="_15-openforensics" tabindex="-1"><a class="header-anchor" href="#_15-openforensics"><span>15 OpenForensics</span></a></h2><h3 id="_15-1-basic-information" tabindex="-1"><a class="header-anchor" href="#_15-1-basic-information"><span>15.1 Basic Information</span></a></h3><ul><li>Introduction: OpenForensics is the first large-scale dataset for <em>multi-face forgery detection and segmentation</em> in complex natural scenes, jointly constructed by the National Institute of Informatics, Japan, the Graduate University for Advanced Studies, and the University of Tokyo. It provides pixel-level fine annotations to support multi-dimensional tasks such as forgery detection, instance segmentation, and forgery boundary identification. <ul><li>Contains 115,325 images with a total of 334,136 faces (an average of 2.9 faces per image), including 160,670 real faces and 173,660 fake faces.</li><li>Divided into training set (44K+ images), validation set (7K+ images), test development set (18K+ images), and test challenge set (45K+ images).</li><li>Image scenes cover both indoor (63.7%) and outdoor (36.3%) environments, with highly diverse face poses, ages, genders, and occlusion conditions, including faces of varying sizes from small to large.</li><li>Fake faces have a resolution of 512×512.</li></ul></li><li>Paper link: <a href="https://arxiv.org/abs/2107.14480" target="_blank" rel="noopener noreferrer">OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild</a></li><li>APA citation: Le, T. N., Nguyen, H. H., Yamagishi, J., &amp; Echizen, I. (2021). Openforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild. In <em>Proceedings of the IEEE/CVF international conference on computer vision</em> (pp. 10117-10127).</li></ul><h3 id="_15-2-download-and-usage" tabindex="-1"><a class="header-anchor" href="#_15-2-download-and-usage"><span>15.2 Download and Usage</span></a></h3><ul><li>Official download link: <a href="https://zenodo.org/records/5528418" target="_blank" rel="noopener noreferrer">https://zenodo.org/records/5528418</a></li><li>The dataset is divided into multiple parts for download. The above link directs to the index for all downloads.</li></ul></div><!--[--><!--]--></div><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link label" href="https://github.com/scu-zjz/IMDLBenCo-doc/edit/main/docs/imdl_data_model_hub/data/IMDLdatasets.md" aria-label="Edit this page on GitHub" rel="noopener noreferrer" target="_blank"><!--[--><!--[--><svg class="edit-icon" viewbox="0 0 1024 1024"><g fill="currentColor"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></g></svg><!--]--><!--]-->Edit this page on GitHub<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-meta-item git-info"><div class="vp-meta-item last-updated"><span class="meta-item-label">Last Updated:: </span><time class="meta-item-info" datetime="2025-04-09T08:18:41.000Z" data-allow-mismatch>4/9/25, 8:18 AM</time></div><div class="vp-meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: mxch1122@126.com">Ma Xiaochen (马晓晨)</span><!--[-->, <!--]--><!--]--><!--[--><span class="contributor" title="email: 98306351+Sylence8@users.noreply.github.com">Sylence8</span><!----><!--]--><!--]--></span></div></div></footer><nav class="vp-page-nav" aria-label="page navigation"><!----><a class="route-link auto-link next" href="/IMDLBenCo-doc/imdl_data_model_hub/data/AIGCdatasets.html" aria-label="AIGC Generated Content Dataset Index"><!--[--><div class="hint">Next <span class="arrow right"></span></div><div class="link"><span class="external-link">AIGC Generated Content Dataset Index</span></div><!--]--></a></nav><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/IMDLBenCo-doc/assets/app-CCtLKDl1.js" defer></script>
  </body>
</html>
